{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Batch Gradient Descent for Softmax Regression without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    total = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps/total\n",
    "\n",
    "def to_one_hot(y):\n",
    "    n_classes = int(y.max() + 1)\n",
    "    m = int(len(y))\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_rate = 0.3\n",
    "n_iterations = 30000\n",
    "epsilon = 1e-7\n",
    "y_train_oh = to_one_hot(y_train)\n",
    "m = len(X_train)\n",
    "\n",
    "n_inputs, n_outputs = X.shape[1], y_train_oh.shape[1]\n",
    "\n",
    "theta_classes = np.random.randn(n_inputs, n_outputs)\n",
    "theta_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112, 2), (2, 3))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, theta_classes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss: $J(\\mathbf{\\Theta}) =\n",
    "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
    "\n",
    "Gradients: $\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    :1.4694396060537207\n",
      "501  :0.6411677511479184\n",
      "1001 :0.5692616874923935\n",
      "1501 :0.5415018314518816\n",
      "2001 :0.5273438196878488\n",
      "2501 :0.5190001374151647\n",
      "3001 :0.5136273518325545\n",
      "3501 :0.5099546739406066\n",
      "4001 :0.5073342102951518\n",
      "4501 :0.5054034100590711\n",
      "5001 :0.5039448229452618\n",
      "5501 :0.5028208392580653\n",
      "6001 :0.5019405784104956\n",
      "6501 :0.5012419010752486\n",
      "7001 :0.5006810809117711\n",
      "7501 :0.5002265963254476\n",
      "8001 :0.4998552521953034\n",
      "8501 :0.49954967624107155\n",
      "9001 :0.4992966563599956\n",
      "9501 :0.49908600887096594\n",
      "10001:0.4989097912621639\n",
      "10501:0.4987617439709632\n",
      "11001:0.4986368877436945\n",
      "11501:0.49853122873465905\n",
      "12001:0.49844153951808795\n",
      "12501:0.4983651944321182\n",
      "13001:0.4983000443655908\n",
      "13501:0.49824432055212803\n",
      "14001:0.49819655995132334\n",
      "14501:0.49815554687076563\n",
      "15001:0.4981202669297386\n",
      "15501:0.49808987048872916\n",
      "16001:0.49806364340143755\n",
      "16501:0.49804098347645803\n",
      "17001:0.49802138142400887\n",
      "17501:0.4980044053500499\n",
      "18001:0.49798968807419314\n",
      "18501:0.49797691670890387\n",
      "19001:0.49796582405970696\n",
      "19501:0.49795618149953774\n",
      "20001:0.4979477930423201\n",
      "20501:0.4979404903966366\n",
      "21001:0.4979341288238525\n",
      "21501:0.4979285836592329\n",
      "22001:0.4979237473815508\n",
      "22501:0.4979195271380848\n",
      "23001:0.49791584264900074\n",
      "23501:0.4979126244287884\n",
      "24001:0.49790981227346803\n",
      "24501:0.4979073539712019\n",
      "25001:0.4979052042011954\n",
      "25501:0.49790332359169465\n",
      "26001:0.49790167791272505\n",
      "26501:0.497900237383198\n",
      "27001:0.4978989760752985\n",
      "27501:0.49789787140178127\n",
      "28001:0.49789690367405315\n",
      "28501:0.49789605572080636\n",
      "29001:0.4978953125585209\n",
      "29501:0.4978946611064804\n"
     ]
    }
   ],
   "source": [
    "best_loss = np.infty\n",
    "times_in_a_row = 3\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(theta_classes)\n",
    "    p_hat = softmax(logits)\n",
    "    loss = -np.mean(np.sum(y_train_oh * np.log(p_hat + epsilon), axis=1))# loss function that i have no idea how to do\n",
    "    error = p_hat - y_train_oh\n",
    "    if iteration % 500 == 1:\n",
    "        print(\"{:<5}:{}\".format(iteration, loss))\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        times_in_a_row = 3\n",
    "    else:\n",
    "        times_in_a_row -= 1\n",
    "    if times_in_a_row == 0:\n",
    "        print(\"early stopping!\")\n",
    "        break\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    theta_classes += -learn_rate * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
