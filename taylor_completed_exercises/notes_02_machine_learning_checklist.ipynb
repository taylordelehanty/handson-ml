{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Checklist\n",
    "\n",
    "    1) frame the problem and look at the big picture\n",
    "    2) get the data\n",
    "    3) explore the data to gain insights\n",
    "    4) prepare the data to better expose the underlying data patterns to ML algorithms\n",
    "    5) explore many different models and short-list the best ones\n",
    "    6) fine-tune your models and combine them into a great solution\n",
    "    7) present your solution\n",
    "    8) launch, monitor, and maintain your system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Frame the problem and look at the big picture\n",
    "\n",
    "    1) define the objective in business terms\n",
    "    2) how will your solution be used\n",
    "    3) what are the current solutions/workarounds (if any)\n",
    "    4) how should you frame this problem (super-/unsupervised, on-/offline, instance-/model-based, etc.)\n",
    "    5) how should performance be measured\n",
    "    6) is the performance measure aligned with the business objective\n",
    "    7) what would be the minimum performance needed to reach the business objective\n",
    "    8) what are comparable problems? can you reuse experience or tools\n",
    "    9) is human expertise available\n",
    "    10) how would you solve the problem manually\n",
    "    11) list the assumptions you (or others) have made so far\n",
    "    12) verify assumptions if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Get the data\n",
    "    1) list the data you need and how much you need\n",
    "    2) find and document where you can get that data\n",
    "    3) check how much space it will take\n",
    "    4) check legal obligations and get authorization if necessary\n",
    "    5) get access authorizations\n",
    "    6) create a workspace (with enough storage space)\n",
    "    7) get the data\n",
    "    8) convert the data to a format you can easily manipulate without changing the data itself, i.e. keep an original copy\n",
    "    9) ensure sensitive information is deleted or protected (e.g. anonymized)\n",
    "    10) check the size and type of the data (time series, sample, geographical, etc.)\n",
    "    11) sample a test set, put it aside and never look at it (no data snooping!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Explore the data\n",
    "#### Note: try to get insights from a field expert for these steps\n",
    "    1) create a copy of the data for exploration (sampling it down to a manageable size if necessary)\n",
    "    2) create a Jupyter notebook to keep a record of your data exploration\n",
    "    3) study each attribute and its characteristics:\n",
    "        - name\n",
    "        - type (categorical, int/float, un-/bounded, text, structured, etc.)\n",
    "        - percent of missing values\n",
    "        - noisiness and type of noise (stochastic, outliers, rounding errors, etc.)\n",
    "        - possibly userful for the task?\n",
    "        - type of distribution (Gaussian, uniform, logarithmic, etc.)\n",
    "    4) for supervised learning tasks, identify the target attribute(s)\n",
    "    5) visualize the data\n",
    "    6) study the correlations between attributes\n",
    "    7) study how you would solve the problem manually\n",
    "    8) identify the promising transformations you may want to apply\n",
    "    9) identify extra data that would be useful\n",
    "    10) document what you have learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Prepare the data\n",
    "#### Notes:\n",
    "    - make a copy of the original data every time you work with it\n",
    "    - write functions for all data transformations you apply for 5 reasons:\n",
    "        1) to use on future copies of datasets (copies from following note 1)\n",
    "        2) to use in future projects\n",
    "        3) to clean and prepare the test set\n",
    "        4) to clean and prepare new data instances once your solution is live\n",
    "        5) to make it easy to treat your preparation choices as hyperparameters\n",
    "        6) to ensure consistency through the whole pipeline\n",
    "        \n",
    "    1) data cleaning:\n",
    "        - fix or remove outliers (optional)\n",
    "        - fill in missing values (e.g. with zero, mean, median, ...) or drop their rows or columns\n",
    "    2) feature selection (optional):\n",
    "        - drop the attributes that are not useful\n",
    "    3) feature engineering, where appropriate:\n",
    "        - discretize continuous features\n",
    "        - decompose features (e.g. categorical, date/time, etc.)\n",
    "        - add promising transformations of features (e.g. log(x), sqrt(x), x^2, etc.)\n",
    "        - aggregate features into promising new features\n",
    "    4) feature scaling:\n",
    "        - standardize or normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Short-List promising methods\n",
    "#### Notes:\n",
    "    - if the data is huge, maybe sample smaller training sets to train in more reasonable time (**be aware** this penalizes complex models such as large neural nets or Random Forests)\n",
    "    - automate what you can\n",
    "    \n",
    "    1) train many quick models from different categories (e.g. linear, naive Bayes, SVM, Random Forests, neural nets, etc.) using standard parameters\n",
    "    2) measure the performance for each model using n-fold cross-validation and compute the mean and standard deviation of the performance measure on the n-folds. compare.\n",
    "    3) analyze the most significant variables for each algorithm\n",
    "    4) analyze the types of errors the models make. what data would a human have used to avoid these errors?\n",
    "    5) have a quick round of feature selection and engineering\n",
    "    6) have one or two more quick iterations of the five previous steps\n",
    "    7) short-list the top three to five most promising models, preferring models that make different types of errors\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Fine-Tune the System\n",
    "#### Notes:\n",
    "    - use as much data as possible for this step\n",
    "    - automate what you can\n",
    "    \n",
    "    1) fine-tune hyperparameters using cross-validation\n",
    "        - treat your data transformations as hyperparameters, especially when you are not sure about them (e.g. should I replace missing values with zero or mean?)\n",
    "        - prefer random search over grid search if there are many hyperparameters\n",
    "    2) try ensemble methods because combining models will often perform better than if done individually\n",
    "    3) once you are confident about the model, measure its performance on the test set to estimate the generalization error\n",
    "\n",
    "## Warning\n",
    "Don't tweak your model after measuring the generalization error: you will start overfitting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) Present your solution\n",
    "    1) Document what you have done\n",
    "    2) Create a nice presentation\n",
    "        - highlight the big picture first\n",
    "    3) explain why your solution achieves the business objective\n",
    "    4) don't forget to present interesting points you noticed along the way\n",
    "        - describe what worked and what didn't\n",
    "        - list your assumptions and your system's limitations\n",
    "    5) ensure your key findings are communicated through **beautiful** visualizations or easy-to-remember statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch!\n",
    "    1) get your solution ready for production (plug into production data inputs, write unit tests, etc.)\n",
    "    2) write monitoring code to check your system's live performance at regular intervals and trigger alerts when necessary\n",
    "        - as data evolves, models tend to experience \"slow degradation\"\n",
    "        - measuring performance may require a human pipeline (e.g. crowdsourcing)\n",
    "        - monitor input quality (e.g. a malfunctioning sensor or a data source becoming stale)\n",
    "    3) retrain your models on a regular basis on fresh data (automate as much as possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
